{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created this notebook in order to practice writing and running deep learning algorithms on my own EEG data. For information about the data used in this project, see the following page: https://lpljacob.github.io/word_priming/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from scipy import stats\n",
    "from os.path import dirname, join as pjoin\n",
    "from skimage.measure import block_reduce\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below declares variables used in loading and processing the data.\n",
    "\n",
    "The EEG files used in this project have been pre-processed using EEGLAB (details: https://www.biorxiv.org/content/10.1101/862516v1) and are saved as .mat files (one separate file per subject) containing 3D arrays. \n",
    "\n",
    "Arrays consist of these dimensions:\n",
    "- electrodes (64);\n",
    "- timepoints (700, in ms);\n",
    "- trials (variable per subject, as trials with eye blinks, eye movements, and other artifacts were discarded).\n",
    "\n",
    "Each subject also needs a separate vector with condition labels; vector length must be equal to the third dimension of corresponding EEG data array.\n",
    "\n",
    "Based on domain knowledge and the SVM results reported in the paper linked above, I am selecting only the timepoints from 200-500 ms. (see https://lpljacob.github.io/word_priming/ for an explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'C:\\_Files\\svm files'\n",
    "n = 20 # number of subjects\n",
    "electrodes = 64\n",
    "starting_point = 200 # starting point (ms) used for analyses\n",
    "points = 300 # how many points (ms) to use for analyses\n",
    "timewind_size = 10 # if larger than 1, average across points (ms)\n",
    "\n",
    "# preallocate space\n",
    "eeg_data = np.empty(shape=(electrodes,int(points/timewind_size),0))\n",
    "trial_info = np.empty(shape=(1,0))\n",
    "sub_info = np.empty(shape=(1,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below loads and processes the data according to the parameters declared above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,n+1): # iterate through subjects\n",
    "\n",
    "    # load and prepare EEG trial data (X)\n",
    "    mat_file = sio.loadmat(pjoin(folder, 'subdata' + str(i) + '.mat'))\n",
    "    mat_data = mat_file['datasave']\n",
    "\n",
    "    # selects specific timepoints and discards the rest\n",
    "    crop_data = mat_data[:,starting_point:starting_point+points,:] \n",
    "    \n",
    "    # if timewind_size > 1, average across timepoints\n",
    "    avg_data = block_reduce(crop_data, block_size=(1,timewind_size,1), \n",
    "                            func=np.mean, cval=np.mean(crop_data))\n",
    "    \n",
    "    # normalize trial\n",
    "    normdata = np.zeros(shape=avg_data.shape)\n",
    "                         \n",
    "    for j in range(avg_data.shape[-1]):\n",
    "        normdata[:,:,j] = (\n",
    "            avg_data[:,:,j] - np.mean(avg_data[:,:,j].flatten())) / np.std(avg_data[:,:,j].flatten())\n",
    "\n",
    "    # load and prepare trial info (y)\n",
    "    mat_info = sio.loadmat(pjoin(folder, 'trialinfo' + str(i) + '.mat'))\n",
    "    mat_infodata = mat_info['trialsave']\n",
    "\n",
    "    # store everything\n",
    "    eeg_data = np.append(eeg_data, normdata,axis=2)\n",
    "    trial_info = np.append(trial_info, mat_infodata)\n",
    "    sub_info = np.append(sub_info, np.repeat(i, avg_data.shape[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below creates a new file with elements that contain both subject and condition information for each trial. This is used for stratified kfolds; we want conditions and subjects to be counterbalanced across folds.\n",
    "\n",
    "The first part of the code collapses the 16 labels (which stipulate both experimental condition and subject choice) into 8 condition labels, regardless of subject choice. (again, see https://lpljacob.github.io/word_priming/ for information on the experimental paradigm)\n",
    "\n",
    "Finally, we transpose the EEG data (to pytorch specifications, with batch_size first for personal preference) and convert it to the data type we will be using for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain condition information\n",
    "condition_pairs = np.array([13, 14, 15, 16, 9, 10, 11, 12])\n",
    "conditions = trial_info.copy()\n",
    "\n",
    "for i in range (1,9):\n",
    "    conditions[np.where(conditions == condition_pairs[i-1])] = i\n",
    "\n",
    "# combine conditions and sub_info (for stratified kfolds)\n",
    "details = np.chararray((conditions.shape[-1]))\n",
    "details = np.core.defchararray.add(np.char.mod('%d', conditions), np.char.mod('s%d', sub_info))\n",
    "\n",
    "# transpose data to batch_size, seq_len, input_size\n",
    "examples = eeg_data.transpose(2,1,0)\n",
    "examples = examples.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to predict what choice (out of 2 options) the subject has made in each trail. The label file is generated below. \n",
    "\n",
    "Afterward, check if we can train on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain labels\n",
    "binlabels = np.zeros(trial_info.shape)\n",
    "binlabels[trial_info<9] = 1 # 'same' choice\n",
    "binlabels[trial_info>8] = 0 # 'different' choice\n",
    "binlabels = binlabels.astype('float32') \n",
    "\n",
    "# check if we can train on gpu\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "if train_on_gpu:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we declare the model architecture. Given we have timeseries data, I have chosen a recurrent network. The code supports both GRU and LSTM, and allows the user to stipulate the number of hidden units and number of GRU/LSTM layers.\n",
    "\n",
    "This is a binary classification task, so we have a fully-connected layer with output of 1 following our recurrent layer(s), and we apply a sigmoid function to this output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, n_layers, output_size=1, rnntype='gru'):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.type = rnntype\n",
    "        \n",
    "        if rnntype=='gru':\n",
    "            self.rec = nn.GRU(input_size, hidden_size, n_layers, batch_first=True)\n",
    "        elif rnntype == 'lstm':\n",
    "            self.rec = nn.LSTM(input_size, hidden_size, n_layers, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        out, hidden = self.rec(x, hidden)\n",
    "        out = out.contiguous().view(-1, self.hidden_size)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        return sig_out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if self.type=='gru':\n",
    "            hidden = weight.new(self.n_layers, batch_size, self.hidden_size).zero_().to(device)\n",
    "        elif self.type=='lstm':\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_size).zero_().to(device),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_size).zero_().to(device))\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we declare an early stopping class (by [stefanonardo](https://gist.github.com/stefanonardo/693d96ceb2f531fa05db530f3e21517d)). This will interrupt model training based on validation accuracy, preventing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credits: https://gist.github.com/stefanonardo/693d96ceb2f531fa05db530f3e21517d\n",
    "\n",
    "class EarlyStopping(object):\n",
    "    def __init__(self, mode='min', min_delta=0, patience=10, percentage=False):\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = 0\n",
    "        self.is_better = None\n",
    "        self._init_is_better(mode, min_delta, percentage)\n",
    "\n",
    "        if patience == 0:\n",
    "            self.is_better = lambda a, b: True\n",
    "            self.step = lambda a: False\n",
    "\n",
    "    def step(self, metrics):\n",
    "        if self.best is None:\n",
    "            self.best = metrics\n",
    "            return False\n",
    "\n",
    "        if np.isnan(metrics):\n",
    "            return True\n",
    "\n",
    "        if self.is_better(metrics, self.best):\n",
    "            self.num_bad_epochs = 0\n",
    "            self.best = metrics\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _init_is_better(self, mode, min_delta, percentage):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError('mode ' + mode + ' is unknown!')\n",
    "        if not percentage:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - min_delta\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + min_delta\n",
    "        else:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - (\n",
    "                            best * min_delta / 100)\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + (\n",
    "                            best * min_delta / 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the training function.\n",
    "\n",
    "Initial exploration with the model showed it was very prone to overfitting, likely due to the small amount of trials per subject and condition. To remedy this, I've tried to keep the model relatively simple (with one recurrent layer and 16 hidden units).\n",
    "\n",
    "I am also using L1 regularization to encourage a sparse solution, as I believe only a few electrodes (input features) will contain neural activity relevant to the classification task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 10\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle=True)\n",
    "\n",
    "parameters = {'batch_size': 64, 'L1_scale': 0.0005, 'hidden_size': 16, 'n_layers': 1}\n",
    "\n",
    "def train_model(parameters):\n",
    "    \n",
    "    model_type = 'gru'\n",
    "    n_layers = parameters['n_layers']\n",
    "    \n",
    "    epochs = 1000 # arbitrary large number; early stopping will interrupt training much sooner\n",
    "\n",
    "    counter = 0\n",
    "    f_counter = 0\n",
    "\n",
    "    all_accs = []\n",
    "    all_losses = [] \n",
    "\n",
    "    # kfold loop\n",
    "    for train_index, test_index in skf.split(examples, details):\n",
    "        f_counter += 1\n",
    "\n",
    "        # create a new instance of the model\n",
    "        net = RNN(input_size=examples.shape[-1], hidden_size=parameters['hidden_size'], \n",
    "                  n_layers=n_layers, rnntype=model_type)\n",
    "        \n",
    "        # determine loss function and optimizer\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            net.cuda()\n",
    "\n",
    "        # obtain kfold data\n",
    "        X_train, X_val = examples[train_index], examples[test_index]\n",
    "        y_train, y_val = binlabels[train_index], binlabels[test_index]\n",
    "\n",
    "        # convert to tensor\n",
    "        train_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "        val_data = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
    "\n",
    "        # create dataloaders\n",
    "        train_loader = DataLoader(train_data, batch_size=parameters['batch_size'])\n",
    "        val_loader = DataLoader(val_data, batch_size=y_val.shape[-1])\n",
    "\n",
    "        total_correct = 0\n",
    "        total_examples = 0\n",
    "\n",
    "        fold_losses = []\n",
    "        fold_accs = []\n",
    "\n",
    "        es = EarlyStopping(patience=10)\n",
    "\n",
    "        for e in range(epochs):\n",
    "\n",
    "            # training\n",
    "            for inputs, labels in train_loader:             \n",
    "                counter += 1\n",
    "\n",
    "                # initialize hidden state\n",
    "                h = net.init_hidden(inputs.size(0))\n",
    "                net.train()\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                net.zero_grad()\n",
    "\n",
    "                if model_type == 'gru':\n",
    "                    h = h.data\n",
    "                else:\n",
    "                    h = tuple([e.data for e in h])\n",
    "\n",
    "                output, h = net(inputs, h)\n",
    "\n",
    "                # L1 regularization\n",
    "                L1_reg = torch.tensor(0., requires_grad=True)\n",
    "                for name, param in net.named_parameters():\n",
    "                    if 'weight' in name:\n",
    "                        L1_reg = L1_reg + torch.norm(param, 1)\n",
    "                        \n",
    "                loss = criterion(output.squeeze(), labels.float()) + parameters['L1_scale'] * L1_reg\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "            # validation\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in val_loader:\n",
    "\n",
    "                val_h = net.init_hidden(inputs.size(0))\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "                total_correct =+ torch.sum(output.round()==labels).cpu().numpy()\n",
    "                total_examples =+ labels.size(0)\n",
    "\n",
    "            net.train()\n",
    "\n",
    "            fold_losses.append(np.mean(val_losses))\n",
    "            fold_accs.append(total_correct/total_examples)\n",
    "\n",
    "            print(\"Fold: {}/{}...\".format(f_counter, folds),\n",
    "                  \"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}...\".format(np.mean(val_losses)),\n",
    "                  \"Accuracy: {:.6f}\".format(total_correct/total_examples))\n",
    "\n",
    "            # check if we should stop training on this fold\n",
    "            if es.step(np.mean(val_losses)):\n",
    "                all_accs.append(np.max(fold_accs))\n",
    "                all_losses.append(np.min(fold_losses))\n",
    "                print(\"Early stopping now...\",\n",
    "                      \"Min val loss: {:.6f}...\".format(np.min(fold_losses)),\n",
    "                      \"Max acc: {:.6f}\".format(np.max(fold_accs)))\n",
    "                break\n",
    "                \n",
    "    return all_accs, all_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model (which should only take a couple minutes if using a GPU), then look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1/10... Epoch: 1/1000... Loss: 0.751457... Val Loss: 0.692414... Accuracy: 0.530120\n",
      "Fold: 1/10... Epoch: 2/1000... Loss: 0.686209... Val Loss: 0.688675... Accuracy: 0.530120\n",
      "Fold: 1/10... Epoch: 3/1000... Loss: 0.658448... Val Loss: 0.686569... Accuracy: 0.543264\n",
      "Fold: 1/10... Epoch: 4/1000... Loss: 0.643891... Val Loss: 0.685023... Accuracy: 0.541073\n",
      "Fold: 1/10... Epoch: 5/1000... Loss: 0.633489... Val Loss: 0.683737... Accuracy: 0.554217\n",
      "Fold: 1/10... Epoch: 6/1000... Loss: 0.625578... Val Loss: 0.682604... Accuracy: 0.562979\n",
      "Fold: 1/10... Epoch: 7/1000... Loss: 0.619731... Val Loss: 0.681388... Accuracy: 0.565170\n",
      "Fold: 1/10... Epoch: 8/1000... Loss: 0.615376... Val Loss: 0.680056... Accuracy: 0.564074\n",
      "Fold: 1/10... Epoch: 9/1000... Loss: 0.613478... Val Loss: 0.678265... Accuracy: 0.572837\n",
      "Fold: 1/10... Epoch: 10/1000... Loss: 0.609757... Val Loss: 0.676004... Accuracy: 0.570646\n",
      "Fold: 1/10... Epoch: 11/1000... Loss: 0.603917... Val Loss: 0.673128... Accuracy: 0.584885\n",
      "Fold: 1/10... Epoch: 12/1000... Loss: 0.592515... Val Loss: 0.669538... Accuracy: 0.592552\n",
      "Fold: 1/10... Epoch: 13/1000... Loss: 0.574653... Val Loss: 0.665086... Accuracy: 0.602410\n",
      "Fold: 1/10... Epoch: 14/1000... Loss: 0.556888... Val Loss: 0.659919... Accuracy: 0.610077\n",
      "Fold: 1/10... Epoch: 15/1000... Loss: 0.536542... Val Loss: 0.654886... Accuracy: 0.624315\n",
      "Fold: 1/10... Epoch: 16/1000... Loss: 0.516083... Val Loss: 0.650717... Accuracy: 0.626506\n",
      "Fold: 1/10... Epoch: 17/1000... Loss: 0.499350... Val Loss: 0.647570... Accuracy: 0.629792\n",
      "Fold: 1/10... Epoch: 18/1000... Loss: 0.485711... Val Loss: 0.645158... Accuracy: 0.637459\n",
      "Fold: 1/10... Epoch: 19/1000... Loss: 0.475202... Val Loss: 0.643141... Accuracy: 0.639650\n",
      "Fold: 1/10... Epoch: 20/1000... Loss: 0.466847... Val Loss: 0.641342... Accuracy: 0.634173\n",
      "Fold: 1/10... Epoch: 21/1000... Loss: 0.460345... Val Loss: 0.639659... Accuracy: 0.636364\n",
      "Fold: 1/10... Epoch: 22/1000... Loss: 0.454699... Val Loss: 0.638056... Accuracy: 0.638554\n",
      "Fold: 1/10... Epoch: 23/1000... Loss: 0.449343... Val Loss: 0.636508... Accuracy: 0.639650\n",
      "Fold: 1/10... Epoch: 24/1000... Loss: 0.443761... Val Loss: 0.635027... Accuracy: 0.640745\n",
      "Fold: 1/10... Epoch: 25/1000... Loss: 0.438624... Val Loss: 0.633590... Accuracy: 0.641840\n",
      "Fold: 1/10... Epoch: 26/1000... Loss: 0.434213... Val Loss: 0.632186... Accuracy: 0.641840\n",
      "Fold: 1/10... Epoch: 27/1000... Loss: 0.429865... Val Loss: 0.630902... Accuracy: 0.642935\n",
      "Fold: 1/10... Epoch: 28/1000... Loss: 0.426329... Val Loss: 0.629714... Accuracy: 0.642935\n",
      "Fold: 1/10... Epoch: 29/1000... Loss: 0.422892... Val Loss: 0.628656... Accuracy: 0.647317\n",
      "Fold: 1/10... Epoch: 30/1000... Loss: 0.419447... Val Loss: 0.627684... Accuracy: 0.647317\n",
      "Fold: 1/10... Epoch: 31/1000... Loss: 0.416170... Val Loss: 0.626848... Accuracy: 0.645126\n",
      "Fold: 1/10... Epoch: 32/1000... Loss: 0.413265... Val Loss: 0.626146... Accuracy: 0.646221\n",
      "Fold: 1/10... Epoch: 33/1000... Loss: 0.410526... Val Loss: 0.625552... Accuracy: 0.646221\n",
      "Fold: 1/10... Epoch: 34/1000... Loss: 0.407644... Val Loss: 0.625014... Accuracy: 0.648412\n",
      "Fold: 1/10... Epoch: 35/1000... Loss: 0.404703... Val Loss: 0.624541... Accuracy: 0.651698\n",
      "Fold: 1/10... Epoch: 36/1000... Loss: 0.401899... Val Loss: 0.624133... Accuracy: 0.651698\n",
      "Fold: 1/10... Epoch: 37/1000... Loss: 0.399307... Val Loss: 0.623770... Accuracy: 0.652793\n",
      "Fold: 1/10... Epoch: 38/1000... Loss: 0.396774... Val Loss: 0.623436... Accuracy: 0.657174\n",
      "Fold: 1/10... Epoch: 39/1000... Loss: 0.393911... Val Loss: 0.623114... Accuracy: 0.656079\n",
      "Fold: 1/10... Epoch: 40/1000... Loss: 0.391172... Val Loss: 0.622824... Accuracy: 0.652793\n",
      "Fold: 1/10... Epoch: 41/1000... Loss: 0.388947... Val Loss: 0.622545... Accuracy: 0.652793\n",
      "Fold: 1/10... Epoch: 42/1000... Loss: 0.386670... Val Loss: 0.622287... Accuracy: 0.652793\n",
      "Fold: 1/10... Epoch: 43/1000... Loss: 0.384551... Val Loss: 0.622046... Accuracy: 0.652793\n",
      "Fold: 1/10... Epoch: 44/1000... Loss: 0.382500... Val Loss: 0.621813... Accuracy: 0.653888\n",
      "Fold: 1/10... Epoch: 45/1000... Loss: 0.380650... Val Loss: 0.621593... Accuracy: 0.653888\n",
      "Fold: 1/10... Epoch: 46/1000... Loss: 0.378876... Val Loss: 0.621396... Accuracy: 0.654984\n",
      "Fold: 1/10... Epoch: 47/1000... Loss: 0.377461... Val Loss: 0.621210... Accuracy: 0.651698\n",
      "Fold: 1/10... Epoch: 48/1000... Loss: 0.375933... Val Loss: 0.621060... Accuracy: 0.653888\n",
      "Fold: 1/10... Epoch: 49/1000... Loss: 0.374529... Val Loss: 0.620947... Accuracy: 0.654984\n",
      "Fold: 1/10... Epoch: 50/1000... Loss: 0.373242... Val Loss: 0.620859... Accuracy: 0.658269\n",
      "Fold: 1/10... Epoch: 51/1000... Loss: 0.372025... Val Loss: 0.620799... Accuracy: 0.656079\n",
      "Fold: 1/10... Epoch: 52/1000... Loss: 0.370910... Val Loss: 0.620760... Accuracy: 0.658269\n",
      "Fold: 1/10... Epoch: 53/1000... Loss: 0.369591... Val Loss: 0.620737... Accuracy: 0.659365\n",
      "Fold: 1/10... Epoch: 54/1000... Loss: 0.368348... Val Loss: 0.620718... Accuracy: 0.660460\n",
      "Fold: 1/10... Epoch: 55/1000... Loss: 0.367037... Val Loss: 0.620710... Accuracy: 0.660460\n",
      "Fold: 1/10... Epoch: 56/1000... Loss: 0.365379... Val Loss: 0.620702... Accuracy: 0.662651\n",
      "Fold: 1/10... Epoch: 57/1000... Loss: 0.363739... Val Loss: 0.620699... Accuracy: 0.659365\n",
      "Fold: 1/10... Epoch: 58/1000... Loss: 0.362048... Val Loss: 0.620728... Accuracy: 0.659365\n",
      "Fold: 1/10... Epoch: 59/1000... Loss: 0.360492... Val Loss: 0.620539... Accuracy: 0.659365\n",
      "Fold: 1/10... Epoch: 60/1000... Loss: 0.360553... Val Loss: 0.620536... Accuracy: 0.659365\n",
      "Fold: 1/10... Epoch: 61/1000... Loss: 0.356886... Val Loss: 0.620711... Accuracy: 0.654984\n",
      "Fold: 1/10... Epoch: 62/1000... Loss: 0.353598... Val Loss: 0.620999... Accuracy: 0.652793\n",
      "Fold: 1/10... Epoch: 63/1000... Loss: 0.351593... Val Loss: 0.621089... Accuracy: 0.654984\n",
      "Fold: 1/10... Epoch: 64/1000... Loss: 0.349945... Val Loss: 0.621209... Accuracy: 0.659365\n",
      "Fold: 1/10... Epoch: 65/1000... Loss: 0.348401... Val Loss: 0.621341... Accuracy: 0.659365\n",
      "Fold: 1/10... Epoch: 66/1000... Loss: 0.346547... Val Loss: 0.621456... Accuracy: 0.660460\n",
      "Fold: 1/10... Epoch: 67/1000... Loss: 0.344599... Val Loss: 0.621559... Accuracy: 0.660460\n",
      "Fold: 1/10... Epoch: 68/1000... Loss: 0.342742... Val Loss: 0.621672... Accuracy: 0.659365\n",
      "Fold: 1/10... Epoch: 69/1000... Loss: 0.340818... Val Loss: 0.621752... Accuracy: 0.661555\n",
      "Fold: 1/10... Epoch: 70/1000... Loss: 0.337522... Val Loss: 0.621733... Accuracy: 0.663746\n",
      "Early stopping now... Min val loss: 0.620536... Max acc: 0.663746\n",
      "Fold: 2/10... Epoch: 1/1000... Loss: 0.810244... Val Loss: 0.686891... Accuracy: 0.546256\n",
      "Fold: 2/10... Epoch: 2/1000... Loss: 0.735064... Val Loss: 0.684598... Accuracy: 0.557269\n",
      "Fold: 2/10... Epoch: 3/1000... Loss: 0.701044... Val Loss: 0.682851... Accuracy: 0.570485\n",
      "Fold: 2/10... Epoch: 4/1000... Loss: 0.679368... Val Loss: 0.681744... Accuracy: 0.578194\n",
      "Fold: 2/10... Epoch: 5/1000... Loss: 0.666184... Val Loss: 0.680999... Accuracy: 0.571586\n",
      "Fold: 2/10... Epoch: 6/1000... Loss: 0.658852... Val Loss: 0.680296... Accuracy: 0.579295\n",
      "Fold: 2/10... Epoch: 7/1000... Loss: 0.654011... Val Loss: 0.679522... Accuracy: 0.575991\n",
      "Fold: 2/10... Epoch: 8/1000... Loss: 0.649544... Val Loss: 0.678297... Accuracy: 0.571586\n",
      "Fold: 2/10... Epoch: 9/1000... Loss: 0.644185... Val Loss: 0.676536... Accuracy: 0.568282\n",
      "Fold: 2/10... Epoch: 10/1000... Loss: 0.636664... Val Loss: 0.674233... Accuracy: 0.575991\n",
      "Fold: 2/10... Epoch: 11/1000... Loss: 0.625501... Val Loss: 0.671048... Accuracy: 0.578194\n",
      "Fold: 2/10... Epoch: 12/1000... Loss: 0.609507... Val Loss: 0.667221... Accuracy: 0.588106\n",
      "Fold: 2/10... Epoch: 13/1000... Loss: 0.587898... Val Loss: 0.663363... Accuracy: 0.606828\n",
      "Fold: 2/10... Epoch: 14/1000... Loss: 0.566407... Val Loss: 0.659725... Accuracy: 0.611233\n",
      "Fold: 2/10... Epoch: 15/1000... Loss: 0.547884... Val Loss: 0.656526... Accuracy: 0.615639\n",
      "Fold: 2/10... Epoch: 16/1000... Loss: 0.533569... Val Loss: 0.653765... Accuracy: 0.617841\n",
      "Fold: 2/10... Epoch: 17/1000... Loss: 0.522369... Val Loss: 0.651461... Accuracy: 0.614537\n",
      "Fold: 2/10... Epoch: 18/1000... Loss: 0.513176... Val Loss: 0.649675... Accuracy: 0.618943\n",
      "Fold: 2/10... Epoch: 19/1000... Loss: 0.505228... Val Loss: 0.648201... Accuracy: 0.620044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 2/10... Epoch: 20/1000... Loss: 0.498756... Val Loss: 0.646932... Accuracy: 0.620044\n",
      "Fold: 2/10... Epoch: 21/1000... Loss: 0.494184... Val Loss: 0.645780... Accuracy: 0.623348\n",
      "Fold: 2/10... Epoch: 22/1000... Loss: 0.491623... Val Loss: 0.644746... Accuracy: 0.629956\n",
      "Fold: 2/10... Epoch: 23/1000... Loss: 0.490338... Val Loss: 0.643819... Accuracy: 0.629956\n",
      "Fold: 2/10... Epoch: 24/1000... Loss: 0.490543... Val Loss: 0.643043... Accuracy: 0.628855\n",
      "Fold: 2/10... Epoch: 25/1000... Loss: 0.491269... Val Loss: 0.642362... Accuracy: 0.629956\n",
      "Fold: 2/10... Epoch: 26/1000... Loss: 0.492293... Val Loss: 0.641792... Accuracy: 0.631057\n",
      "Fold: 2/10... Epoch: 27/1000... Loss: 0.493287... Val Loss: 0.641329... Accuracy: 0.638767\n",
      "Fold: 2/10... Epoch: 28/1000... Loss: 0.493895... Val Loss: 0.640916... Accuracy: 0.635463\n",
      "Fold: 2/10... Epoch: 29/1000... Loss: 0.494195... Val Loss: 0.640528... Accuracy: 0.639868\n",
      "Fold: 2/10... Epoch: 30/1000... Loss: 0.494521... Val Loss: 0.640189... Accuracy: 0.640969\n",
      "Fold: 2/10... Epoch: 31/1000... Loss: 0.494655... Val Loss: 0.639904... Accuracy: 0.638767\n",
      "Fold: 2/10... Epoch: 32/1000... Loss: 0.494710... Val Loss: 0.639679... Accuracy: 0.640969\n",
      "Fold: 2/10... Epoch: 33/1000... Loss: 0.494649... Val Loss: 0.639544... Accuracy: 0.639868\n",
      "Fold: 2/10... Epoch: 34/1000... Loss: 0.494441... Val Loss: 0.639465... Accuracy: 0.638767\n",
      "Fold: 2/10... Epoch: 35/1000... Loss: 0.494088... Val Loss: 0.639418... Accuracy: 0.640969\n",
      "Fold: 2/10... Epoch: 36/1000... Loss: 0.493672... Val Loss: 0.639405... Accuracy: 0.636564\n",
      "Fold: 2/10... Epoch: 37/1000... Loss: 0.493145... Val Loss: 0.639436... Accuracy: 0.637665\n",
      "Fold: 2/10... Epoch: 38/1000... Loss: 0.492701... Val Loss: 0.639474... Accuracy: 0.637665\n",
      "Fold: 2/10... Epoch: 39/1000... Loss: 0.491786... Val Loss: 0.639539... Accuracy: 0.640969\n",
      "Fold: 2/10... Epoch: 40/1000... Loss: 0.490722... Val Loss: 0.639625... Accuracy: 0.640969\n",
      "Fold: 2/10... Epoch: 41/1000... Loss: 0.489427... Val Loss: 0.639743... Accuracy: 0.642070\n",
      "Fold: 2/10... Epoch: 42/1000... Loss: 0.488108... Val Loss: 0.639876... Accuracy: 0.646476\n",
      "Fold: 2/10... Epoch: 43/1000... Loss: 0.486699... Val Loss: 0.640010... Accuracy: 0.647577\n",
      "Fold: 2/10... Epoch: 44/1000... Loss: 0.485224... Val Loss: 0.640146... Accuracy: 0.644273\n",
      "Fold: 2/10... Epoch: 45/1000... Loss: 0.483688... Val Loss: 0.640280... Accuracy: 0.642070\n",
      "Fold: 2/10... Epoch: 46/1000... Loss: 0.482228... Val Loss: 0.640438... Accuracy: 0.643172\n",
      "Early stopping now... Min val loss: 0.639405... Max acc: 0.647577\n",
      "Fold: 3/10... Epoch: 1/1000... Loss: 0.780355... Val Loss: 0.693534... Accuracy: 0.535635\n",
      "Fold: 3/10... Epoch: 2/1000... Loss: 0.730283... Val Loss: 0.688980... Accuracy: 0.544543\n",
      "Fold: 3/10... Epoch: 3/1000... Loss: 0.709015... Val Loss: 0.687012... Accuracy: 0.567929\n",
      "Fold: 3/10... Epoch: 4/1000... Loss: 0.697403... Val Loss: 0.685736... Accuracy: 0.564588\n",
      "Fold: 3/10... Epoch: 5/1000... Loss: 0.688263... Val Loss: 0.684789... Accuracy: 0.571269\n",
      "Fold: 3/10... Epoch: 6/1000... Loss: 0.680391... Val Loss: 0.683824... Accuracy: 0.570156\n",
      "Fold: 3/10... Epoch: 7/1000... Loss: 0.673083... Val Loss: 0.682669... Accuracy: 0.567929\n",
      "Fold: 3/10... Epoch: 8/1000... Loss: 0.665780... Val Loss: 0.681086... Accuracy: 0.565702\n",
      "Fold: 3/10... Epoch: 9/1000... Loss: 0.656414... Val Loss: 0.678839... Accuracy: 0.566815\n",
      "Fold: 3/10... Epoch: 10/1000... Loss: 0.641743... Val Loss: 0.676554... Accuracy: 0.575724\n",
      "Fold: 3/10... Epoch: 11/1000... Loss: 0.621466... Val Loss: 0.674716... Accuracy: 0.581292\n",
      "Fold: 3/10... Epoch: 12/1000... Loss: 0.601060... Val Loss: 0.673071... Accuracy: 0.587973\n",
      "Fold: 3/10... Epoch: 13/1000... Loss: 0.584674... Val Loss: 0.671461... Accuracy: 0.592428\n",
      "Fold: 3/10... Epoch: 14/1000... Loss: 0.571587... Val Loss: 0.669999... Accuracy: 0.591314\n",
      "Fold: 3/10... Epoch: 15/1000... Loss: 0.560537... Val Loss: 0.668709... Accuracy: 0.593541\n",
      "Fold: 3/10... Epoch: 16/1000... Loss: 0.550681... Val Loss: 0.667476... Accuracy: 0.591314\n",
      "Fold: 3/10... Epoch: 17/1000... Loss: 0.541799... Val Loss: 0.666267... Accuracy: 0.596882\n",
      "Fold: 3/10... Epoch: 18/1000... Loss: 0.533193... Val Loss: 0.665146... Accuracy: 0.603563\n",
      "Fold: 3/10... Epoch: 19/1000... Loss: 0.524993... Val Loss: 0.664104... Accuracy: 0.601336\n",
      "Fold: 3/10... Epoch: 20/1000... Loss: 0.517559... Val Loss: 0.663167... Accuracy: 0.599109\n",
      "Fold: 3/10... Epoch: 21/1000... Loss: 0.510502... Val Loss: 0.662335... Accuracy: 0.600223\n",
      "Fold: 3/10... Epoch: 22/1000... Loss: 0.503598... Val Loss: 0.661593... Accuracy: 0.603563\n",
      "Fold: 3/10... Epoch: 23/1000... Loss: 0.497067... Val Loss: 0.660912... Accuracy: 0.604677\n",
      "Fold: 3/10... Epoch: 24/1000... Loss: 0.490730... Val Loss: 0.660301... Accuracy: 0.604677\n",
      "Fold: 3/10... Epoch: 25/1000... Loss: 0.484986... Val Loss: 0.659719... Accuracy: 0.611359\n",
      "Fold: 3/10... Epoch: 26/1000... Loss: 0.479661... Val Loss: 0.659134... Accuracy: 0.609131\n",
      "Fold: 3/10... Epoch: 27/1000... Loss: 0.474573... Val Loss: 0.658601... Accuracy: 0.609131\n",
      "Fold: 3/10... Epoch: 28/1000... Loss: 0.469807... Val Loss: 0.658121... Accuracy: 0.606904\n",
      "Fold: 3/10... Epoch: 29/1000... Loss: 0.465459... Val Loss: 0.657681... Accuracy: 0.604677\n",
      "Fold: 3/10... Epoch: 30/1000... Loss: 0.461287... Val Loss: 0.657314... Accuracy: 0.605791\n",
      "Fold: 3/10... Epoch: 31/1000... Loss: 0.457452... Val Loss: 0.657042... Accuracy: 0.610245\n",
      "Fold: 3/10... Epoch: 32/1000... Loss: 0.453796... Val Loss: 0.656806... Accuracy: 0.612472\n",
      "Fold: 3/10... Epoch: 33/1000... Loss: 0.450315... Val Loss: 0.656650... Accuracy: 0.615813\n",
      "Fold: 3/10... Epoch: 34/1000... Loss: 0.446816... Val Loss: 0.656571... Accuracy: 0.616927\n",
      "Fold: 3/10... Epoch: 35/1000... Loss: 0.443538... Val Loss: 0.656515... Accuracy: 0.612472\n",
      "Fold: 3/10... Epoch: 36/1000... Loss: 0.440358... Val Loss: 0.656519... Accuracy: 0.616927\n",
      "Fold: 3/10... Epoch: 37/1000... Loss: 0.437451... Val Loss: 0.656598... Accuracy: 0.619154\n",
      "Fold: 3/10... Epoch: 38/1000... Loss: 0.434585... Val Loss: 0.656712... Accuracy: 0.615813\n",
      "Fold: 3/10... Epoch: 39/1000... Loss: 0.431976... Val Loss: 0.656879... Accuracy: 0.614699\n",
      "Fold: 3/10... Epoch: 40/1000... Loss: 0.429495... Val Loss: 0.657061... Accuracy: 0.613586\n",
      "Fold: 3/10... Epoch: 41/1000... Loss: 0.427170... Val Loss: 0.657285... Accuracy: 0.613586\n",
      "Fold: 3/10... Epoch: 42/1000... Loss: 0.425086... Val Loss: 0.657553... Accuracy: 0.610245\n",
      "Fold: 3/10... Epoch: 43/1000... Loss: 0.423000... Val Loss: 0.657859... Accuracy: 0.611359\n",
      "Fold: 3/10... Epoch: 44/1000... Loss: 0.421190... Val Loss: 0.658194... Accuracy: 0.611359\n",
      "Fold: 3/10... Epoch: 45/1000... Loss: 0.419752... Val Loss: 0.658558... Accuracy: 0.616927\n",
      "Early stopping now... Min val loss: 0.656515... Max acc: 0.619154\n",
      "Fold: 4/10... Epoch: 1/1000... Loss: 0.823909... Val Loss: 0.689529... Accuracy: 0.519819\n",
      "Fold: 4/10... Epoch: 2/1000... Loss: 0.748918... Val Loss: 0.687820... Accuracy: 0.530011\n",
      "Fold: 4/10... Epoch: 3/1000... Loss: 0.716754... Val Loss: 0.686085... Accuracy: 0.552661\n",
      "Fold: 4/10... Epoch: 4/1000... Loss: 0.698711... Val Loss: 0.684369... Accuracy: 0.560589\n",
      "Fold: 4/10... Epoch: 5/1000... Loss: 0.685348... Val Loss: 0.682438... Accuracy: 0.559456\n",
      "Fold: 4/10... Epoch: 6/1000... Loss: 0.675140... Val Loss: 0.680213... Accuracy: 0.562854\n",
      "Fold: 4/10... Epoch: 7/1000... Loss: 0.665631... Val Loss: 0.677501... Accuracy: 0.573046\n",
      "Fold: 4/10... Epoch: 8/1000... Loss: 0.655501... Val Loss: 0.674588... Accuracy: 0.582106\n",
      "Fold: 4/10... Epoch: 9/1000... Loss: 0.644440... Val Loss: 0.671975... Accuracy: 0.591166\n",
      "Fold: 4/10... Epoch: 10/1000... Loss: 0.630453... Val Loss: 0.669501... Accuracy: 0.594564\n",
      "Fold: 4/10... Epoch: 11/1000... Loss: 0.613893... Val Loss: 0.667141... Accuracy: 0.594564\n",
      "Fold: 4/10... Epoch: 12/1000... Loss: 0.595968... Val Loss: 0.664825... Accuracy: 0.603624\n",
      "Fold: 4/10... Epoch: 13/1000... Loss: 0.579606... Val Loss: 0.662654... Accuracy: 0.604757\n",
      "Fold: 4/10... Epoch: 14/1000... Loss: 0.566920... Val Loss: 0.660555... Accuracy: 0.596829\n",
      "Fold: 4/10... Epoch: 15/1000... Loss: 0.556953... Val Loss: 0.658406... Accuracy: 0.600227\n",
      "Fold: 4/10... Epoch: 16/1000... Loss: 0.549049... Val Loss: 0.656148... Accuracy: 0.599094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 4/10... Epoch: 17/1000... Loss: 0.542166... Val Loss: 0.653716... Accuracy: 0.602492\n",
      "Fold: 4/10... Epoch: 18/1000... Loss: 0.536111... Val Loss: 0.651092... Accuracy: 0.601359\n",
      "Fold: 4/10... Epoch: 19/1000... Loss: 0.530252... Val Loss: 0.648444... Accuracy: 0.616082\n",
      "Fold: 4/10... Epoch: 20/1000... Loss: 0.524409... Val Loss: 0.646052... Accuracy: 0.625142\n",
      "Fold: 4/10... Epoch: 21/1000... Loss: 0.518517... Val Loss: 0.644070... Accuracy: 0.633069\n",
      "Fold: 4/10... Epoch: 22/1000... Loss: 0.512870... Val Loss: 0.642609... Accuracy: 0.630804\n",
      "Fold: 4/10... Epoch: 23/1000... Loss: 0.507194... Val Loss: 0.641570... Accuracy: 0.635334\n",
      "Fold: 4/10... Epoch: 24/1000... Loss: 0.501518... Val Loss: 0.640837... Accuracy: 0.638732\n",
      "Fold: 4/10... Epoch: 25/1000... Loss: 0.495942... Val Loss: 0.640264... Accuracy: 0.640997\n",
      "Fold: 4/10... Epoch: 26/1000... Loss: 0.490358... Val Loss: 0.639854... Accuracy: 0.634202\n",
      "Fold: 4/10... Epoch: 27/1000... Loss: 0.484725... Val Loss: 0.639616... Accuracy: 0.638732\n",
      "Fold: 4/10... Epoch: 28/1000... Loss: 0.478916... Val Loss: 0.639530... Accuracy: 0.638732\n",
      "Fold: 4/10... Epoch: 29/1000... Loss: 0.473314... Val Loss: 0.639540... Accuracy: 0.639864\n",
      "Fold: 4/10... Epoch: 30/1000... Loss: 0.467828... Val Loss: 0.639663... Accuracy: 0.638732\n",
      "Fold: 4/10... Epoch: 31/1000... Loss: 0.462556... Val Loss: 0.639842... Accuracy: 0.634202\n",
      "Fold: 4/10... Epoch: 32/1000... Loss: 0.457589... Val Loss: 0.640060... Accuracy: 0.637599\n",
      "Fold: 4/10... Epoch: 33/1000... Loss: 0.452982... Val Loss: 0.640308... Accuracy: 0.635334\n",
      "Fold: 4/10... Epoch: 34/1000... Loss: 0.448742... Val Loss: 0.640586... Accuracy: 0.634202\n",
      "Fold: 4/10... Epoch: 35/1000... Loss: 0.444911... Val Loss: 0.640837... Accuracy: 0.631937\n",
      "Fold: 4/10... Epoch: 36/1000... Loss: 0.441569... Val Loss: 0.641097... Accuracy: 0.629672\n",
      "Fold: 4/10... Epoch: 37/1000... Loss: 0.438321... Val Loss: 0.641356... Accuracy: 0.631937\n",
      "Fold: 4/10... Epoch: 38/1000... Loss: 0.435438... Val Loss: 0.641615... Accuracy: 0.629672\n",
      "Early stopping now... Min val loss: 0.639530... Max acc: 0.640997\n",
      "Fold: 5/10... Epoch: 1/1000... Loss: 0.818335... Val Loss: 0.690572... Accuracy: 0.520046\n",
      "Fold: 5/10... Epoch: 2/1000... Loss: 0.753056... Val Loss: 0.689211... Accuracy: 0.520046\n",
      "Fold: 5/10... Epoch: 3/1000... Loss: 0.724899... Val Loss: 0.688441... Accuracy: 0.533792\n",
      "Fold: 5/10... Epoch: 4/1000... Loss: 0.710058... Val Loss: 0.687739... Accuracy: 0.542955\n",
      "Fold: 5/10... Epoch: 5/1000... Loss: 0.698529... Val Loss: 0.686600... Accuracy: 0.548683\n",
      "Fold: 5/10... Epoch: 6/1000... Loss: 0.688701... Val Loss: 0.684772... Accuracy: 0.556701\n",
      "Fold: 5/10... Epoch: 7/1000... Loss: 0.677723... Val Loss: 0.681620... Accuracy: 0.560137\n",
      "Fold: 5/10... Epoch: 8/1000... Loss: 0.664042... Val Loss: 0.676265... Accuracy: 0.583047\n",
      "Fold: 5/10... Epoch: 9/1000... Loss: 0.646878... Val Loss: 0.669661... Accuracy: 0.593356\n",
      "Fold: 5/10... Epoch: 10/1000... Loss: 0.629866... Val Loss: 0.663917... Accuracy: 0.605956\n",
      "Fold: 5/10... Epoch: 11/1000... Loss: 0.615923... Val Loss: 0.659659... Accuracy: 0.617411\n",
      "Fold: 5/10... Epoch: 12/1000... Loss: 0.604005... Val Loss: 0.656571... Accuracy: 0.626575\n",
      "Fold: 5/10... Epoch: 13/1000... Loss: 0.593496... Val Loss: 0.654219... Accuracy: 0.633448\n",
      "Fold: 5/10... Epoch: 14/1000... Loss: 0.584074... Val Loss: 0.652315... Accuracy: 0.636884\n",
      "Fold: 5/10... Epoch: 15/1000... Loss: 0.575493... Val Loss: 0.650755... Accuracy: 0.633448\n",
      "Fold: 5/10... Epoch: 16/1000... Loss: 0.567742... Val Loss: 0.649484... Accuracy: 0.630011\n",
      "Fold: 5/10... Epoch: 17/1000... Loss: 0.560765... Val Loss: 0.648446... Accuracy: 0.627721\n",
      "Fold: 5/10... Epoch: 18/1000... Loss: 0.554274... Val Loss: 0.647616... Accuracy: 0.632302\n",
      "Fold: 5/10... Epoch: 19/1000... Loss: 0.548360... Val Loss: 0.646958... Accuracy: 0.628866\n",
      "Fold: 5/10... Epoch: 20/1000... Loss: 0.542791... Val Loss: 0.646388... Accuracy: 0.624284\n",
      "Fold: 5/10... Epoch: 21/1000... Loss: 0.537677... Val Loss: 0.645873... Accuracy: 0.620848\n",
      "Fold: 5/10... Epoch: 22/1000... Loss: 0.533107... Val Loss: 0.645353... Accuracy: 0.619702\n",
      "Fold: 5/10... Epoch: 23/1000... Loss: 0.529165... Val Loss: 0.644801... Accuracy: 0.623139\n",
      "Fold: 5/10... Epoch: 24/1000... Loss: 0.525623... Val Loss: 0.644289... Accuracy: 0.623139\n",
      "Fold: 5/10... Epoch: 25/1000... Loss: 0.522679... Val Loss: 0.643721... Accuracy: 0.626575\n",
      "Fold: 5/10... Epoch: 26/1000... Loss: 0.519505... Val Loss: 0.643090... Accuracy: 0.623139\n",
      "Fold: 5/10... Epoch: 27/1000... Loss: 0.516958... Val Loss: 0.643098... Accuracy: 0.617411\n",
      "Fold: 5/10... Epoch: 28/1000... Loss: 0.514551... Val Loss: 0.642655... Accuracy: 0.615120\n",
      "Fold: 5/10... Epoch: 29/1000... Loss: 0.512746... Val Loss: 0.642392... Accuracy: 0.618557\n",
      "Fold: 5/10... Epoch: 30/1000... Loss: 0.510869... Val Loss: 0.642115... Accuracy: 0.624284\n",
      "Fold: 5/10... Epoch: 31/1000... Loss: 0.508992... Val Loss: 0.641873... Accuracy: 0.625430\n",
      "Fold: 5/10... Epoch: 32/1000... Loss: 0.507199... Val Loss: 0.641655... Accuracy: 0.624284\n",
      "Fold: 5/10... Epoch: 33/1000... Loss: 0.505583... Val Loss: 0.641505... Accuracy: 0.624284\n",
      "Fold: 5/10... Epoch: 34/1000... Loss: 0.503898... Val Loss: 0.641357... Accuracy: 0.625430\n",
      "Fold: 5/10... Epoch: 35/1000... Loss: 0.502114... Val Loss: 0.641243... Accuracy: 0.621993\n",
      "Fold: 5/10... Epoch: 36/1000... Loss: 0.500335... Val Loss: 0.641096... Accuracy: 0.623139\n",
      "Fold: 5/10... Epoch: 37/1000... Loss: 0.498517... Val Loss: 0.641058... Accuracy: 0.623139\n",
      "Fold: 5/10... Epoch: 38/1000... Loss: 0.496734... Val Loss: 0.641290... Accuracy: 0.623139\n",
      "Fold: 5/10... Epoch: 39/1000... Loss: 0.495214... Val Loss: 0.641144... Accuracy: 0.628866\n",
      "Fold: 5/10... Epoch: 40/1000... Loss: 0.494059... Val Loss: 0.641203... Accuracy: 0.630011\n",
      "Fold: 5/10... Epoch: 41/1000... Loss: 0.492519... Val Loss: 0.641285... Accuracy: 0.628866\n",
      "Fold: 5/10... Epoch: 42/1000... Loss: 0.490968... Val Loss: 0.641275... Accuracy: 0.631157\n",
      "Fold: 5/10... Epoch: 43/1000... Loss: 0.489361... Val Loss: 0.641374... Accuracy: 0.631157\n",
      "Fold: 5/10... Epoch: 44/1000... Loss: 0.487807... Val Loss: 0.641434... Accuracy: 0.628866\n",
      "Fold: 5/10... Epoch: 45/1000... Loss: 0.486429... Val Loss: 0.641455... Accuracy: 0.627721\n",
      "Fold: 5/10... Epoch: 46/1000... Loss: 0.485049... Val Loss: 0.641518... Accuracy: 0.625430\n",
      "Fold: 5/10... Epoch: 47/1000... Loss: 0.483762... Val Loss: 0.641542... Accuracy: 0.625430\n",
      "Early stopping now... Min val loss: 0.641058... Max acc: 0.636884\n",
      "Fold: 6/10... Epoch: 1/1000... Loss: 0.774913... Val Loss: 0.682340... Accuracy: 0.574739\n",
      "Fold: 6/10... Epoch: 2/1000... Loss: 0.718039... Val Loss: 0.680289... Accuracy: 0.575898\n",
      "Fold: 6/10... Epoch: 3/1000... Loss: 0.697732... Val Loss: 0.678954... Accuracy: 0.574739\n",
      "Fold: 6/10... Epoch: 4/1000... Loss: 0.685292... Val Loss: 0.677980... Accuracy: 0.590962\n",
      "Fold: 6/10... Epoch: 5/1000... Loss: 0.674887... Val Loss: 0.677050... Accuracy: 0.593279\n",
      "Fold: 6/10... Epoch: 6/1000... Loss: 0.665604... Val Loss: 0.676321... Accuracy: 0.593279\n",
      "Fold: 6/10... Epoch: 7/1000... Loss: 0.655356... Val Loss: 0.675409... Accuracy: 0.602549\n",
      "Fold: 6/10... Epoch: 8/1000... Loss: 0.644178... Val Loss: 0.674102... Accuracy: 0.599073\n",
      "Fold: 6/10... Epoch: 9/1000... Loss: 0.631314... Val Loss: 0.671924... Accuracy: 0.601390\n",
      "Fold: 6/10... Epoch: 10/1000... Loss: 0.616512... Val Loss: 0.669152... Accuracy: 0.607184\n",
      "Fold: 6/10... Epoch: 11/1000... Loss: 0.599985... Val Loss: 0.666389... Accuracy: 0.614137\n",
      "Fold: 6/10... Epoch: 12/1000... Loss: 0.583465... Val Loss: 0.664260... Accuracy: 0.609502\n",
      "Fold: 6/10... Epoch: 13/1000... Loss: 0.568041... Val Loss: 0.662653... Accuracy: 0.608343\n",
      "Fold: 6/10... Epoch: 14/1000... Loss: 0.555156... Val Loss: 0.661435... Accuracy: 0.612978\n",
      "Fold: 6/10... Epoch: 15/1000... Loss: 0.545078... Val Loss: 0.660607... Accuracy: 0.609502\n",
      "Fold: 6/10... Epoch: 16/1000... Loss: 0.536969... Val Loss: 0.660107... Accuracy: 0.607184\n",
      "Fold: 6/10... Epoch: 17/1000... Loss: 0.530201... Val Loss: 0.659871... Accuracy: 0.610660\n",
      "Fold: 6/10... Epoch: 18/1000... Loss: 0.524463... Val Loss: 0.659856... Accuracy: 0.609502\n",
      "Fold: 6/10... Epoch: 19/1000... Loss: 0.519399... Val Loss: 0.660015... Accuracy: 0.611819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 6/10... Epoch: 20/1000... Loss: 0.514862... Val Loss: 0.660228... Accuracy: 0.603708\n",
      "Fold: 6/10... Epoch: 21/1000... Loss: 0.510954... Val Loss: 0.660433... Accuracy: 0.602549\n",
      "Fold: 6/10... Epoch: 22/1000... Loss: 0.507265... Val Loss: 0.660663... Accuracy: 0.603708\n",
      "Fold: 6/10... Epoch: 23/1000... Loss: 0.503687... Val Loss: 0.660939... Accuracy: 0.601390\n",
      "Fold: 6/10... Epoch: 24/1000... Loss: 0.499968... Val Loss: 0.661147... Accuracy: 0.602549\n",
      "Fold: 6/10... Epoch: 25/1000... Loss: 0.496394... Val Loss: 0.661322... Accuracy: 0.603708\n",
      "Fold: 6/10... Epoch: 26/1000... Loss: 0.493209... Val Loss: 0.661401... Accuracy: 0.604867\n",
      "Fold: 6/10... Epoch: 27/1000... Loss: 0.490403... Val Loss: 0.661437... Accuracy: 0.600232\n",
      "Fold: 6/10... Epoch: 28/1000... Loss: 0.487451... Val Loss: 0.661251... Accuracy: 0.600232\n",
      "Early stopping now... Min val loss: 0.659856... Max acc: 0.614137\n",
      "Fold: 7/10... Epoch: 1/1000... Loss: 0.830648... Val Loss: 0.691585... Accuracy: 0.540828\n",
      "Fold: 7/10... Epoch: 2/1000... Loss: 0.756754... Val Loss: 0.687967... Accuracy: 0.540828\n",
      "Fold: 7/10... Epoch: 3/1000... Loss: 0.723673... Val Loss: 0.685348... Accuracy: 0.549112\n",
      "Fold: 7/10... Epoch: 4/1000... Loss: 0.701834... Val Loss: 0.683622... Accuracy: 0.553846\n",
      "Fold: 7/10... Epoch: 5/1000... Loss: 0.686185... Val Loss: 0.682257... Accuracy: 0.566864\n",
      "Fold: 7/10... Epoch: 6/1000... Loss: 0.673397... Val Loss: 0.681117... Accuracy: 0.571598\n",
      "Fold: 7/10... Epoch: 7/1000... Loss: 0.663633... Val Loss: 0.680113... Accuracy: 0.575148\n",
      "Fold: 7/10... Epoch: 8/1000... Loss: 0.651734... Val Loss: 0.678725... Accuracy: 0.572781\n",
      "Fold: 7/10... Epoch: 9/1000... Loss: 0.636985... Val Loss: 0.676738... Accuracy: 0.576331\n",
      "Fold: 7/10... Epoch: 10/1000... Loss: 0.617395... Val Loss: 0.674221... Accuracy: 0.581065\n",
      "Fold: 7/10... Epoch: 11/1000... Loss: 0.594019... Val Loss: 0.671891... Accuracy: 0.589349\n",
      "Fold: 7/10... Epoch: 12/1000... Loss: 0.569984... Val Loss: 0.669780... Accuracy: 0.596450\n",
      "Fold: 7/10... Epoch: 13/1000... Loss: 0.549954... Val Loss: 0.667961... Accuracy: 0.595266\n",
      "Fold: 7/10... Epoch: 14/1000... Loss: 0.536028... Val Loss: 0.666524... Accuracy: 0.596450\n",
      "Fold: 7/10... Epoch: 15/1000... Loss: 0.526162... Val Loss: 0.665538... Accuracy: 0.594083\n",
      "Fold: 7/10... Epoch: 16/1000... Loss: 0.519615... Val Loss: 0.664935... Accuracy: 0.596450\n",
      "Fold: 7/10... Epoch: 17/1000... Loss: 0.515324... Val Loss: 0.664490... Accuracy: 0.602367\n",
      "Fold: 7/10... Epoch: 18/1000... Loss: 0.512245... Val Loss: 0.664165... Accuracy: 0.598817\n",
      "Fold: 7/10... Epoch: 19/1000... Loss: 0.510005... Val Loss: 0.663906... Accuracy: 0.598817\n",
      "Fold: 7/10... Epoch: 20/1000... Loss: 0.508768... Val Loss: 0.663689... Accuracy: 0.598817\n",
      "Fold: 7/10... Epoch: 21/1000... Loss: 0.508078... Val Loss: 0.663454... Accuracy: 0.601183\n",
      "Fold: 7/10... Epoch: 22/1000... Loss: 0.507978... Val Loss: 0.663099... Accuracy: 0.600000\n",
      "Fold: 7/10... Epoch: 23/1000... Loss: 0.508854... Val Loss: 0.662727... Accuracy: 0.601183\n",
      "Fold: 7/10... Epoch: 24/1000... Loss: 0.510116... Val Loss: 0.662426... Accuracy: 0.600000\n",
      "Fold: 7/10... Epoch: 25/1000... Loss: 0.511862... Val Loss: 0.662112... Accuracy: 0.601183\n",
      "Fold: 7/10... Epoch: 26/1000... Loss: 0.513763... Val Loss: 0.661781... Accuracy: 0.601183\n",
      "Fold: 7/10... Epoch: 27/1000... Loss: 0.515879... Val Loss: 0.661439... Accuracy: 0.607101\n",
      "Fold: 7/10... Epoch: 28/1000... Loss: 0.518343... Val Loss: 0.661089... Accuracy: 0.601183\n",
      "Fold: 7/10... Epoch: 29/1000... Loss: 0.520807... Val Loss: 0.660746... Accuracy: 0.601183\n",
      "Fold: 7/10... Epoch: 30/1000... Loss: 0.523221... Val Loss: 0.660400... Accuracy: 0.601183\n",
      "Fold: 7/10... Epoch: 31/1000... Loss: 0.525509... Val Loss: 0.660041... Accuracy: 0.601183\n",
      "Fold: 7/10... Epoch: 32/1000... Loss: 0.528033... Val Loss: 0.659663... Accuracy: 0.597633\n",
      "Fold: 7/10... Epoch: 33/1000... Loss: 0.530367... Val Loss: 0.659315... Accuracy: 0.595266\n",
      "Fold: 7/10... Epoch: 34/1000... Loss: 0.532407... Val Loss: 0.658958... Accuracy: 0.595266\n",
      "Fold: 7/10... Epoch: 35/1000... Loss: 0.534115... Val Loss: 0.658629... Accuracy: 0.592899\n",
      "Fold: 7/10... Epoch: 36/1000... Loss: 0.536803... Val Loss: 0.658406... Accuracy: 0.591716\n",
      "Fold: 7/10... Epoch: 37/1000... Loss: 0.537006... Val Loss: 0.658176... Accuracy: 0.592899\n",
      "Fold: 7/10... Epoch: 38/1000... Loss: 0.538475... Val Loss: 0.657880... Accuracy: 0.596450\n",
      "Fold: 7/10... Epoch: 39/1000... Loss: 0.538877... Val Loss: 0.657659... Accuracy: 0.592899\n",
      "Fold: 7/10... Epoch: 40/1000... Loss: 0.539556... Val Loss: 0.657478... Accuracy: 0.590533\n",
      "Fold: 7/10... Epoch: 41/1000... Loss: 0.540316... Val Loss: 0.657305... Accuracy: 0.591716\n",
      "Fold: 7/10... Epoch: 42/1000... Loss: 0.540838... Val Loss: 0.657266... Accuracy: 0.589349\n",
      "Fold: 7/10... Epoch: 43/1000... Loss: 0.540472... Val Loss: 0.657049... Accuracy: 0.590533\n",
      "Fold: 7/10... Epoch: 44/1000... Loss: 0.539846... Val Loss: 0.656926... Accuracy: 0.591716\n",
      "Fold: 7/10... Epoch: 45/1000... Loss: 0.538719... Val Loss: 0.656828... Accuracy: 0.592899\n",
      "Fold: 7/10... Epoch: 46/1000... Loss: 0.537575... Val Loss: 0.656725... Accuracy: 0.590533\n",
      "Fold: 7/10... Epoch: 47/1000... Loss: 0.536192... Val Loss: 0.656610... Accuracy: 0.592899\n",
      "Fold: 7/10... Epoch: 48/1000... Loss: 0.534456... Val Loss: 0.656495... Accuracy: 0.591716\n",
      "Fold: 7/10... Epoch: 49/1000... Loss: 0.532869... Val Loss: 0.656382... Accuracy: 0.590533\n",
      "Fold: 7/10... Epoch: 50/1000... Loss: 0.531112... Val Loss: 0.656297... Accuracy: 0.590533\n",
      "Fold: 7/10... Epoch: 51/1000... Loss: 0.529309... Val Loss: 0.656216... Accuracy: 0.586982\n",
      "Fold: 7/10... Epoch: 52/1000... Loss: 0.527337... Val Loss: 0.656137... Accuracy: 0.586982\n",
      "Fold: 7/10... Epoch: 53/1000... Loss: 0.525322... Val Loss: 0.656052... Accuracy: 0.588166\n",
      "Fold: 7/10... Epoch: 54/1000... Loss: 0.523413... Val Loss: 0.655985... Accuracy: 0.588166\n",
      "Fold: 7/10... Epoch: 55/1000... Loss: 0.521020... Val Loss: 0.655873... Accuracy: 0.586982\n",
      "Fold: 7/10... Epoch: 56/1000... Loss: 0.518905... Val Loss: 0.655815... Accuracy: 0.586982\n",
      "Fold: 7/10... Epoch: 57/1000... Loss: 0.516377... Val Loss: 0.655725... Accuracy: 0.585799\n",
      "Fold: 7/10... Epoch: 58/1000... Loss: 0.514013... Val Loss: 0.655673... Accuracy: 0.588166\n",
      "Fold: 7/10... Epoch: 59/1000... Loss: 0.511058... Val Loss: 0.655626... Accuracy: 0.590533\n",
      "Fold: 7/10... Epoch: 60/1000... Loss: 0.508200... Val Loss: 0.655619... Accuracy: 0.588166\n",
      "Fold: 7/10... Epoch: 61/1000... Loss: 0.503592... Val Loss: 0.655607... Accuracy: 0.588166\n",
      "Fold: 7/10... Epoch: 62/1000... Loss: 0.503079... Val Loss: 0.655415... Accuracy: 0.590533\n",
      "Fold: 7/10... Epoch: 63/1000... Loss: 0.500490... Val Loss: 0.655349... Accuracy: 0.589349\n",
      "Fold: 7/10... Epoch: 64/1000... Loss: 0.496534... Val Loss: 0.655285... Accuracy: 0.586982\n",
      "Fold: 7/10... Epoch: 65/1000... Loss: 0.492622... Val Loss: 0.655192... Accuracy: 0.586982\n",
      "Fold: 7/10... Epoch: 66/1000... Loss: 0.490136... Val Loss: 0.655222... Accuracy: 0.588166\n",
      "Fold: 7/10... Epoch: 67/1000... Loss: 0.485747... Val Loss: 0.655228... Accuracy: 0.589349\n",
      "Fold: 7/10... Epoch: 68/1000... Loss: 0.479863... Val Loss: 0.654999... Accuracy: 0.584615\n",
      "Fold: 7/10... Epoch: 69/1000... Loss: 0.480476... Val Loss: 0.655078... Accuracy: 0.590533\n",
      "Fold: 7/10... Epoch: 70/1000... Loss: 0.471751... Val Loss: 0.654826... Accuracy: 0.588166\n",
      "Fold: 7/10... Epoch: 71/1000... Loss: 0.473085... Val Loss: 0.654847... Accuracy: 0.591716\n",
      "Fold: 7/10... Epoch: 72/1000... Loss: 0.467443... Val Loss: 0.654699... Accuracy: 0.590533\n",
      "Fold: 7/10... Epoch: 73/1000... Loss: 0.463081... Val Loss: 0.654613... Accuracy: 0.594083\n",
      "Fold: 7/10... Epoch: 74/1000... Loss: 0.458609... Val Loss: 0.654356... Accuracy: 0.594083\n",
      "Fold: 7/10... Epoch: 75/1000... Loss: 0.460157... Val Loss: 0.654327... Accuracy: 0.597633\n",
      "Fold: 7/10... Epoch: 76/1000... Loss: 0.452150... Val Loss: 0.654249... Accuracy: 0.596450\n",
      "Fold: 7/10... Epoch: 77/1000... Loss: 0.448182... Val Loss: 0.654086... Accuracy: 0.598817\n",
      "Fold: 7/10... Epoch: 78/1000... Loss: 0.444414... Val Loss: 0.653908... Accuracy: 0.603550\n",
      "Fold: 7/10... Epoch: 79/1000... Loss: 0.440826... Val Loss: 0.653746... Accuracy: 0.603550\n",
      "Fold: 7/10... Epoch: 80/1000... Loss: 0.436945... Val Loss: 0.653570... Accuracy: 0.605917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 7/10... Epoch: 81/1000... Loss: 0.433052... Val Loss: 0.653409... Accuracy: 0.607101\n",
      "Fold: 7/10... Epoch: 82/1000... Loss: 0.429250... Val Loss: 0.653223... Accuracy: 0.609467\n",
      "Fold: 7/10... Epoch: 83/1000... Loss: 0.425407... Val Loss: 0.653050... Accuracy: 0.609467\n",
      "Fold: 7/10... Epoch: 84/1000... Loss: 0.421672... Val Loss: 0.652858... Accuracy: 0.609467\n",
      "Fold: 7/10... Epoch: 85/1000... Loss: 0.417924... Val Loss: 0.652654... Accuracy: 0.610651\n",
      "Fold: 7/10... Epoch: 86/1000... Loss: 0.414400... Val Loss: 0.652466... Accuracy: 0.610651\n",
      "Fold: 7/10... Epoch: 87/1000... Loss: 0.410476... Val Loss: 0.652231... Accuracy: 0.611834\n",
      "Fold: 7/10... Epoch: 88/1000... Loss: 0.404597... Val Loss: 0.652050... Accuracy: 0.611834\n",
      "Fold: 7/10... Epoch: 89/1000... Loss: 0.398548... Val Loss: 0.651540... Accuracy: 0.611834\n",
      "Fold: 7/10... Epoch: 90/1000... Loss: 0.399144... Val Loss: 0.651709... Accuracy: 0.611834\n",
      "Fold: 7/10... Epoch: 91/1000... Loss: 0.395789... Val Loss: 0.651516... Accuracy: 0.611834\n",
      "Fold: 7/10... Epoch: 92/1000... Loss: 0.392698... Val Loss: 0.651198... Accuracy: 0.611834\n",
      "Fold: 7/10... Epoch: 93/1000... Loss: 0.388517... Val Loss: 0.650986... Accuracy: 0.613018\n",
      "Fold: 7/10... Epoch: 94/1000... Loss: 0.382244... Val Loss: 0.650570... Accuracy: 0.611834\n",
      "Fold: 7/10... Epoch: 95/1000... Loss: 0.379544... Val Loss: 0.650070... Accuracy: 0.615385\n",
      "Fold: 7/10... Epoch: 96/1000... Loss: 0.377535... Val Loss: 0.650121... Accuracy: 0.614201\n",
      "Fold: 7/10... Epoch: 97/1000... Loss: 0.373924... Val Loss: 0.650210... Accuracy: 0.615385\n",
      "Fold: 7/10... Epoch: 98/1000... Loss: 0.368887... Val Loss: 0.649728... Accuracy: 0.616568\n",
      "Fold: 7/10... Epoch: 99/1000... Loss: 0.368244... Val Loss: 0.649504... Accuracy: 0.617751\n",
      "Fold: 7/10... Epoch: 100/1000... Loss: 0.363806... Val Loss: 0.649343... Accuracy: 0.615385\n",
      "Fold: 7/10... Epoch: 101/1000... Loss: 0.358292... Val Loss: 0.648865... Accuracy: 0.617751\n",
      "Fold: 7/10... Epoch: 102/1000... Loss: 0.357484... Val Loss: 0.648719... Accuracy: 0.616568\n",
      "Fold: 7/10... Epoch: 103/1000... Loss: 0.355552... Val Loss: 0.648928... Accuracy: 0.614201\n",
      "Fold: 7/10... Epoch: 104/1000... Loss: 0.352612... Val Loss: 0.648741... Accuracy: 0.613018\n",
      "Fold: 7/10... Epoch: 105/1000... Loss: 0.350159... Val Loss: 0.648577... Accuracy: 0.611834\n",
      "Fold: 7/10... Epoch: 106/1000... Loss: 0.347484... Val Loss: 0.648457... Accuracy: 0.611834\n",
      "Fold: 7/10... Epoch: 107/1000... Loss: 0.344689... Val Loss: 0.648390... Accuracy: 0.610651\n",
      "Fold: 7/10... Epoch: 108/1000... Loss: 0.341721... Val Loss: 0.648367... Accuracy: 0.611834\n",
      "Fold: 7/10... Epoch: 109/1000... Loss: 0.338929... Val Loss: 0.648384... Accuracy: 0.613018\n",
      "Fold: 7/10... Epoch: 110/1000... Loss: 0.336110... Val Loss: 0.648445... Accuracy: 0.614201\n",
      "Fold: 7/10... Epoch: 111/1000... Loss: 0.333362... Val Loss: 0.648540... Accuracy: 0.614201\n",
      "Fold: 7/10... Epoch: 112/1000... Loss: 0.330289... Val Loss: 0.648696... Accuracy: 0.616568\n",
      "Fold: 7/10... Epoch: 113/1000... Loss: 0.325466... Val Loss: 0.648705... Accuracy: 0.617751\n",
      "Fold: 7/10... Epoch: 114/1000... Loss: 0.321575... Val Loss: 0.648984... Accuracy: 0.616568\n",
      "Fold: 7/10... Epoch: 115/1000... Loss: 0.317337... Val Loss: 0.648839... Accuracy: 0.616568\n",
      "Fold: 7/10... Epoch: 116/1000... Loss: 0.320299... Val Loss: 0.649526... Accuracy: 0.615385\n",
      "Fold: 7/10... Epoch: 117/1000... Loss: 0.318289... Val Loss: 0.649989... Accuracy: 0.613018\n",
      "Fold: 7/10... Epoch: 118/1000... Loss: 0.315637... Val Loss: 0.650437... Accuracy: 0.613018\n",
      "Early stopping now... Min val loss: 0.648367... Max acc: 0.617751\n",
      "Fold: 8/10... Epoch: 1/1000... Loss: 0.837165... Val Loss: 0.689231... Accuracy: 0.524213\n",
      "Fold: 8/10... Epoch: 2/1000... Loss: 0.764653... Val Loss: 0.686157... Accuracy: 0.538741\n",
      "Fold: 8/10... Epoch: 3/1000... Loss: 0.734894... Val Loss: 0.683738... Accuracy: 0.550847\n",
      "Fold: 8/10... Epoch: 4/1000... Loss: 0.716459... Val Loss: 0.681829... Accuracy: 0.553269\n",
      "Fold: 8/10... Epoch: 5/1000... Loss: 0.704394... Val Loss: 0.680222... Accuracy: 0.561743\n",
      "Fold: 8/10... Epoch: 6/1000... Loss: 0.694423... Val Loss: 0.678306... Accuracy: 0.566586\n",
      "Fold: 8/10... Epoch: 7/1000... Loss: 0.684302... Val Loss: 0.675930... Accuracy: 0.576271\n",
      "Fold: 8/10... Epoch: 8/1000... Loss: 0.673653... Val Loss: 0.672939... Accuracy: 0.588378\n",
      "Fold: 8/10... Epoch: 9/1000... Loss: 0.663704... Val Loss: 0.669065... Accuracy: 0.598063\n",
      "Fold: 8/10... Epoch: 10/1000... Loss: 0.655098... Val Loss: 0.664355... Accuracy: 0.601695\n",
      "Fold: 8/10... Epoch: 11/1000... Loss: 0.646798... Val Loss: 0.659281... Accuracy: 0.607748\n",
      "Fold: 8/10... Epoch: 12/1000... Loss: 0.638690... Val Loss: 0.654703... Accuracy: 0.612591\n",
      "Fold: 8/10... Epoch: 13/1000... Loss: 0.630662... Val Loss: 0.650842... Accuracy: 0.611380\n",
      "Fold: 8/10... Epoch: 14/1000... Loss: 0.623550... Val Loss: 0.647702... Accuracy: 0.611380\n",
      "Fold: 8/10... Epoch: 15/1000... Loss: 0.616452... Val Loss: 0.645054... Accuracy: 0.613801\n",
      "Fold: 8/10... Epoch: 16/1000... Loss: 0.609345... Val Loss: 0.642752... Accuracy: 0.627119\n",
      "Fold: 8/10... Epoch: 17/1000... Loss: 0.602309... Val Loss: 0.640687... Accuracy: 0.630751\n",
      "Fold: 8/10... Epoch: 18/1000... Loss: 0.595325... Val Loss: 0.638731... Accuracy: 0.636804\n",
      "Fold: 8/10... Epoch: 19/1000... Loss: 0.588669... Val Loss: 0.636810... Accuracy: 0.639225\n",
      "Fold: 8/10... Epoch: 20/1000... Loss: 0.582408... Val Loss: 0.634857... Accuracy: 0.645278\n",
      "Fold: 8/10... Epoch: 21/1000... Loss: 0.576711... Val Loss: 0.632901... Accuracy: 0.647700\n",
      "Fold: 8/10... Epoch: 22/1000... Loss: 0.571393... Val Loss: 0.630992... Accuracy: 0.647700\n",
      "Fold: 8/10... Epoch: 23/1000... Loss: 0.566314... Val Loss: 0.629119... Accuracy: 0.653753\n",
      "Fold: 8/10... Epoch: 24/1000... Loss: 0.560970... Val Loss: 0.627341... Accuracy: 0.653753\n",
      "Fold: 8/10... Epoch: 25/1000... Loss: 0.555403... Val Loss: 0.625683... Accuracy: 0.654964\n",
      "Fold: 8/10... Epoch: 26/1000... Loss: 0.549531... Val Loss: 0.624226... Accuracy: 0.657385\n",
      "Fold: 8/10... Epoch: 27/1000... Loss: 0.543525... Val Loss: 0.622971... Accuracy: 0.654964\n",
      "Fold: 8/10... Epoch: 28/1000... Loss: 0.537303... Val Loss: 0.621885... Accuracy: 0.659806\n",
      "Fold: 8/10... Epoch: 29/1000... Loss: 0.531076... Val Loss: 0.621003... Accuracy: 0.659806\n",
      "Fold: 8/10... Epoch: 30/1000... Loss: 0.525368... Val Loss: 0.620310... Accuracy: 0.663438\n",
      "Fold: 8/10... Epoch: 31/1000... Loss: 0.520059... Val Loss: 0.619771... Accuracy: 0.665860\n",
      "Fold: 8/10... Epoch: 32/1000... Loss: 0.515493... Val Loss: 0.619356... Accuracy: 0.668281\n",
      "Fold: 8/10... Epoch: 33/1000... Loss: 0.511270... Val Loss: 0.618985... Accuracy: 0.663438\n",
      "Fold: 8/10... Epoch: 34/1000... Loss: 0.507501... Val Loss: 0.618707... Accuracy: 0.663438\n",
      "Fold: 8/10... Epoch: 35/1000... Loss: 0.504000... Val Loss: 0.618470... Accuracy: 0.665860\n",
      "Fold: 8/10... Epoch: 36/1000... Loss: 0.501046... Val Loss: 0.618272... Accuracy: 0.665860\n",
      "Fold: 8/10... Epoch: 37/1000... Loss: 0.498599... Val Loss: 0.618151... Accuracy: 0.668281\n",
      "Fold: 8/10... Epoch: 38/1000... Loss: 0.496495... Val Loss: 0.618081... Accuracy: 0.664649\n",
      "Fold: 8/10... Epoch: 39/1000... Loss: 0.494714... Val Loss: 0.618061... Accuracy: 0.664649\n",
      "Fold: 8/10... Epoch: 40/1000... Loss: 0.493090... Val Loss: 0.618072... Accuracy: 0.664649\n",
      "Fold: 8/10... Epoch: 41/1000... Loss: 0.491677... Val Loss: 0.618132... Accuracy: 0.658596\n",
      "Fold: 8/10... Epoch: 42/1000... Loss: 0.490443... Val Loss: 0.618213... Accuracy: 0.656174\n",
      "Fold: 8/10... Epoch: 43/1000... Loss: 0.489394... Val Loss: 0.618326... Accuracy: 0.656174\n",
      "Fold: 8/10... Epoch: 44/1000... Loss: 0.488388... Val Loss: 0.618452... Accuracy: 0.657385\n",
      "Fold: 8/10... Epoch: 45/1000... Loss: 0.487335... Val Loss: 0.618623... Accuracy: 0.656174\n",
      "Fold: 8/10... Epoch: 46/1000... Loss: 0.486520... Val Loss: 0.618838... Accuracy: 0.656174\n",
      "Fold: 8/10... Epoch: 47/1000... Loss: 0.485623... Val Loss: 0.619065... Accuracy: 0.653753\n",
      "Fold: 8/10... Epoch: 48/1000... Loss: 0.484943... Val Loss: 0.619277... Accuracy: 0.653753\n",
      "Fold: 8/10... Epoch: 49/1000... Loss: 0.484246... Val Loss: 0.619500... Accuracy: 0.653753\n",
      "Early stopping now... Min val loss: 0.618061... Max acc: 0.668281\n",
      "Fold: 9/10... Epoch: 1/1000... Loss: 0.795154... Val Loss: 0.693666... Accuracy: 0.510533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 9/10... Epoch: 2/1000... Loss: 0.731501... Val Loss: 0.690465... Accuracy: 0.525403\n",
      "Fold: 9/10... Epoch: 3/1000... Loss: 0.708383... Val Loss: 0.688333... Accuracy: 0.532838\n",
      "Fold: 9/10... Epoch: 4/1000... Loss: 0.695967... Val Loss: 0.687338... Accuracy: 0.536555\n",
      "Fold: 9/10... Epoch: 5/1000... Loss: 0.686817... Val Loss: 0.686607... Accuracy: 0.550186\n",
      "Fold: 9/10... Epoch: 6/1000... Loss: 0.679047... Val Loss: 0.685783... Accuracy: 0.558860\n",
      "Fold: 9/10... Epoch: 7/1000... Loss: 0.672224... Val Loss: 0.684473... Accuracy: 0.555143\n",
      "Fold: 9/10... Epoch: 8/1000... Loss: 0.664995... Val Loss: 0.682536... Accuracy: 0.571252\n",
      "Fold: 9/10... Epoch: 9/1000... Loss: 0.657685... Val Loss: 0.680077... Accuracy: 0.583643\n",
      "Fold: 9/10... Epoch: 10/1000... Loss: 0.649585... Val Loss: 0.677145... Accuracy: 0.574969\n",
      "Fold: 9/10... Epoch: 11/1000... Loss: 0.640037... Val Loss: 0.674079... Accuracy: 0.581165\n",
      "Fold: 9/10... Epoch: 12/1000... Loss: 0.630390... Val Loss: 0.671178... Accuracy: 0.587361\n",
      "Fold: 9/10... Epoch: 13/1000... Loss: 0.621328... Val Loss: 0.668681... Accuracy: 0.583643\n",
      "Fold: 9/10... Epoch: 14/1000... Loss: 0.613018... Val Loss: 0.666369... Accuracy: 0.593556\n",
      "Fold: 9/10... Epoch: 15/1000... Loss: 0.604865... Val Loss: 0.664282... Accuracy: 0.593556\n",
      "Fold: 9/10... Epoch: 16/1000... Loss: 0.597009... Val Loss: 0.662414... Accuracy: 0.589839\n",
      "Fold: 9/10... Epoch: 17/1000... Loss: 0.589562... Val Loss: 0.660688... Accuracy: 0.598513\n",
      "Fold: 9/10... Epoch: 18/1000... Loss: 0.582231... Val Loss: 0.659130... Accuracy: 0.596035\n",
      "Fold: 9/10... Epoch: 19/1000... Loss: 0.575307... Val Loss: 0.657699... Accuracy: 0.599752\n",
      "Fold: 9/10... Epoch: 20/1000... Loss: 0.568631... Val Loss: 0.656444... Accuracy: 0.594796\n",
      "Fold: 9/10... Epoch: 21/1000... Loss: 0.562402... Val Loss: 0.655401... Accuracy: 0.598513\n",
      "Fold: 9/10... Epoch: 22/1000... Loss: 0.556791... Val Loss: 0.654553... Accuracy: 0.596035\n",
      "Fold: 9/10... Epoch: 23/1000... Loss: 0.551955... Val Loss: 0.653805... Accuracy: 0.594796\n",
      "Fold: 9/10... Epoch: 24/1000... Loss: 0.547781... Val Loss: 0.653150... Accuracy: 0.598513\n",
      "Fold: 9/10... Epoch: 25/1000... Loss: 0.544427... Val Loss: 0.652561... Accuracy: 0.598513\n",
      "Fold: 9/10... Epoch: 26/1000... Loss: 0.541810... Val Loss: 0.652079... Accuracy: 0.593556\n",
      "Fold: 9/10... Epoch: 27/1000... Loss: 0.539775... Val Loss: 0.651619... Accuracy: 0.597274\n",
      "Fold: 9/10... Epoch: 28/1000... Loss: 0.538101... Val Loss: 0.651197... Accuracy: 0.599752\n",
      "Fold: 9/10... Epoch: 29/1000... Loss: 0.536692... Val Loss: 0.650816... Accuracy: 0.596035\n",
      "Fold: 9/10... Epoch: 30/1000... Loss: 0.535423... Val Loss: 0.650479... Accuracy: 0.597274\n",
      "Fold: 9/10... Epoch: 31/1000... Loss: 0.534137... Val Loss: 0.650178... Accuracy: 0.594796\n",
      "Fold: 9/10... Epoch: 32/1000... Loss: 0.532961... Val Loss: 0.649844... Accuracy: 0.594796\n",
      "Fold: 9/10... Epoch: 33/1000... Loss: 0.531874... Val Loss: 0.649568... Accuracy: 0.597274\n",
      "Fold: 9/10... Epoch: 34/1000... Loss: 0.530863... Val Loss: 0.649345... Accuracy: 0.600991\n",
      "Fold: 9/10... Epoch: 35/1000... Loss: 0.529796... Val Loss: 0.649172... Accuracy: 0.597274\n",
      "Fold: 9/10... Epoch: 36/1000... Loss: 0.528839... Val Loss: 0.649010... Accuracy: 0.596035\n",
      "Fold: 9/10... Epoch: 37/1000... Loss: 0.527498... Val Loss: 0.648982... Accuracy: 0.599752\n",
      "Fold: 9/10... Epoch: 38/1000... Loss: 0.525196... Val Loss: 0.649384... Accuracy: 0.597274\n",
      "Fold: 9/10... Epoch: 39/1000... Loss: 0.524595... Val Loss: 0.649375... Accuracy: 0.597274\n",
      "Fold: 9/10... Epoch: 40/1000... Loss: 0.524232... Val Loss: 0.648848... Accuracy: 0.599752\n",
      "Fold: 9/10... Epoch: 41/1000... Loss: 0.523373... Val Loss: 0.648682... Accuracy: 0.600991\n",
      "Fold: 9/10... Epoch: 42/1000... Loss: 0.522403... Val Loss: 0.648630... Accuracy: 0.605948\n",
      "Fold: 9/10... Epoch: 43/1000... Loss: 0.521301... Val Loss: 0.648617... Accuracy: 0.609665\n",
      "Fold: 9/10... Epoch: 44/1000... Loss: 0.520366... Val Loss: 0.648594... Accuracy: 0.610905\n",
      "Fold: 9/10... Epoch: 45/1000... Loss: 0.519381... Val Loss: 0.648530... Accuracy: 0.610905\n",
      "Fold: 9/10... Epoch: 46/1000... Loss: 0.518481... Val Loss: 0.648454... Accuracy: 0.610905\n",
      "Fold: 9/10... Epoch: 47/1000... Loss: 0.517496... Val Loss: 0.648397... Accuracy: 0.612144\n",
      "Fold: 9/10... Epoch: 48/1000... Loss: 0.516510... Val Loss: 0.648357... Accuracy: 0.610905\n",
      "Fold: 9/10... Epoch: 49/1000... Loss: 0.515531... Val Loss: 0.648337... Accuracy: 0.610905\n",
      "Fold: 9/10... Epoch: 50/1000... Loss: 0.514470... Val Loss: 0.648332... Accuracy: 0.610905\n",
      "Fold: 9/10... Epoch: 51/1000... Loss: 0.513166... Val Loss: 0.648432... Accuracy: 0.614622\n",
      "Fold: 9/10... Epoch: 52/1000... Loss: 0.511794... Val Loss: 0.648458... Accuracy: 0.617100\n",
      "Fold: 9/10... Epoch: 53/1000... Loss: 0.511122... Val Loss: 0.648326... Accuracy: 0.617100\n",
      "Fold: 9/10... Epoch: 54/1000... Loss: 0.510536... Val Loss: 0.648133... Accuracy: 0.614622\n",
      "Fold: 9/10... Epoch: 55/1000... Loss: 0.509743... Val Loss: 0.648005... Accuracy: 0.615861\n",
      "Fold: 9/10... Epoch: 56/1000... Loss: 0.508896... Val Loss: 0.647932... Accuracy: 0.615861\n",
      "Fold: 9/10... Epoch: 57/1000... Loss: 0.508050... Val Loss: 0.647910... Accuracy: 0.613383\n",
      "Fold: 9/10... Epoch: 58/1000... Loss: 0.507455... Val Loss: 0.647819... Accuracy: 0.613383\n",
      "Fold: 9/10... Epoch: 59/1000... Loss: 0.504891... Val Loss: 0.648004... Accuracy: 0.617100\n",
      "Fold: 9/10... Epoch: 60/1000... Loss: 0.504939... Val Loss: 0.648002... Accuracy: 0.615861\n",
      "Fold: 9/10... Epoch: 61/1000... Loss: 0.504260... Val Loss: 0.647826... Accuracy: 0.615861\n",
      "Fold: 9/10... Epoch: 62/1000... Loss: 0.502973... Val Loss: 0.647912... Accuracy: 0.618340\n",
      "Fold: 9/10... Epoch: 63/1000... Loss: 0.502073... Val Loss: 0.647888... Accuracy: 0.618340\n",
      "Fold: 9/10... Epoch: 64/1000... Loss: 0.501576... Val Loss: 0.647791... Accuracy: 0.617100\n",
      "Fold: 9/10... Epoch: 65/1000... Loss: 0.500842... Val Loss: 0.647615... Accuracy: 0.618340\n",
      "Fold: 9/10... Epoch: 66/1000... Loss: 0.499667... Val Loss: 0.647710... Accuracy: 0.620818\n",
      "Fold: 9/10... Epoch: 67/1000... Loss: 0.498662... Val Loss: 0.647894... Accuracy: 0.620818\n",
      "Fold: 9/10... Epoch: 68/1000... Loss: 0.498382... Val Loss: 0.647661... Accuracy: 0.624535\n",
      "Fold: 9/10... Epoch: 69/1000... Loss: 0.497896... Val Loss: 0.647456... Accuracy: 0.624535\n",
      "Fold: 9/10... Epoch: 70/1000... Loss: 0.497319... Val Loss: 0.647214... Accuracy: 0.620818\n",
      "Fold: 9/10... Epoch: 71/1000... Loss: 0.494950... Val Loss: 0.647442... Accuracy: 0.622057\n",
      "Fold: 9/10... Epoch: 72/1000... Loss: 0.494183... Val Loss: 0.647641... Accuracy: 0.624535\n",
      "Fold: 9/10... Epoch: 73/1000... Loss: 0.493282... Val Loss: 0.647704... Accuracy: 0.625774\n",
      "Fold: 9/10... Epoch: 74/1000... Loss: 0.493315... Val Loss: 0.647247... Accuracy: 0.624535\n",
      "Fold: 9/10... Epoch: 75/1000... Loss: 0.491324... Val Loss: 0.647554... Accuracy: 0.627014\n",
      "Fold: 9/10... Epoch: 76/1000... Loss: 0.491159... Val Loss: 0.647595... Accuracy: 0.629492\n",
      "Fold: 9/10... Epoch: 77/1000... Loss: 0.490256... Val Loss: 0.647620... Accuracy: 0.629492\n",
      "Fold: 9/10... Epoch: 78/1000... Loss: 0.489694... Val Loss: 0.647682... Accuracy: 0.625774\n",
      "Fold: 9/10... Epoch: 79/1000... Loss: 0.488994... Val Loss: 0.647768... Accuracy: 0.627014\n",
      "Fold: 9/10... Epoch: 80/1000... Loss: 0.488059... Val Loss: 0.647864... Accuracy: 0.628253\n",
      "Early stopping now... Min val loss: 0.647214... Max acc: 0.629492\n",
      "Fold: 10/10... Epoch: 1/1000... Loss: 0.837759... Val Loss: 0.689187... Accuracy: 0.523077\n",
      "Fold: 10/10... Epoch: 2/1000... Loss: 0.775482... Val Loss: 0.686684... Accuracy: 0.537179\n",
      "Fold: 10/10... Epoch: 3/1000... Loss: 0.744399... Val Loss: 0.684652... Accuracy: 0.542308\n",
      "Fold: 10/10... Epoch: 4/1000... Loss: 0.724816... Val Loss: 0.682587... Accuracy: 0.555128\n",
      "Fold: 10/10... Epoch: 5/1000... Loss: 0.710141... Val Loss: 0.680792... Accuracy: 0.561538\n",
      "Fold: 10/10... Epoch: 6/1000... Loss: 0.700353... Val Loss: 0.679060... Accuracy: 0.569231\n",
      "Fold: 10/10... Epoch: 7/1000... Loss: 0.690604... Val Loss: 0.677045... Accuracy: 0.578205\n",
      "Fold: 10/10... Epoch: 8/1000... Loss: 0.680661... Val Loss: 0.674790... Accuracy: 0.570513\n",
      "Fold: 10/10... Epoch: 9/1000... Loss: 0.667868... Val Loss: 0.671766... Accuracy: 0.580769\n",
      "Fold: 10/10... Epoch: 10/1000... Loss: 0.650151... Val Loss: 0.667750... Accuracy: 0.593590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 10/10... Epoch: 11/1000... Loss: 0.624863... Val Loss: 0.662969... Accuracy: 0.596154\n",
      "Fold: 10/10... Epoch: 12/1000... Loss: 0.592551... Val Loss: 0.658307... Accuracy: 0.602564\n",
      "Fold: 10/10... Epoch: 13/1000... Loss: 0.563597... Val Loss: 0.654526... Accuracy: 0.602564\n",
      "Fold: 10/10... Epoch: 14/1000... Loss: 0.542757... Val Loss: 0.651903... Accuracy: 0.606410\n",
      "Fold: 10/10... Epoch: 15/1000... Loss: 0.528591... Val Loss: 0.650086... Accuracy: 0.610256\n",
      "Fold: 10/10... Epoch: 16/1000... Loss: 0.518163... Val Loss: 0.648748... Accuracy: 0.617949\n",
      "Fold: 10/10... Epoch: 17/1000... Loss: 0.510779... Val Loss: 0.647632... Accuracy: 0.617949\n",
      "Fold: 10/10... Epoch: 18/1000... Loss: 0.505451... Val Loss: 0.646632... Accuracy: 0.614103\n",
      "Fold: 10/10... Epoch: 19/1000... Loss: 0.501689... Val Loss: 0.645742... Accuracy: 0.615385\n",
      "Fold: 10/10... Epoch: 20/1000... Loss: 0.499210... Val Loss: 0.644898... Accuracy: 0.612821\n",
      "Fold: 10/10... Epoch: 21/1000... Loss: 0.496949... Val Loss: 0.644069... Accuracy: 0.608974\n",
      "Fold: 10/10... Epoch: 22/1000... Loss: 0.495256... Val Loss: 0.643318... Accuracy: 0.608974\n",
      "Fold: 10/10... Epoch: 23/1000... Loss: 0.493917... Val Loss: 0.642587... Accuracy: 0.610256\n",
      "Fold: 10/10... Epoch: 24/1000... Loss: 0.492847... Val Loss: 0.641870... Accuracy: 0.608974\n",
      "Fold: 10/10... Epoch: 25/1000... Loss: 0.492271... Val Loss: 0.641174... Accuracy: 0.611538\n",
      "Fold: 10/10... Epoch: 26/1000... Loss: 0.491928... Val Loss: 0.640473... Accuracy: 0.617949\n",
      "Fold: 10/10... Epoch: 27/1000... Loss: 0.491940... Val Loss: 0.639779... Accuracy: 0.619231\n",
      "Fold: 10/10... Epoch: 28/1000... Loss: 0.492311... Val Loss: 0.639066... Accuracy: 0.621795\n",
      "Fold: 10/10... Epoch: 29/1000... Loss: 0.493027... Val Loss: 0.638327... Accuracy: 0.621795\n",
      "Fold: 10/10... Epoch: 30/1000... Loss: 0.494284... Val Loss: 0.637563... Accuracy: 0.621795\n",
      "Fold: 10/10... Epoch: 31/1000... Loss: 0.495543... Val Loss: 0.636782... Accuracy: 0.621795\n",
      "Fold: 10/10... Epoch: 32/1000... Loss: 0.496857... Val Loss: 0.635959... Accuracy: 0.620513\n",
      "Fold: 10/10... Epoch: 33/1000... Loss: 0.498268... Val Loss: 0.635101... Accuracy: 0.619231\n",
      "Fold: 10/10... Epoch: 34/1000... Loss: 0.499907... Val Loss: 0.634252... Accuracy: 0.619231\n",
      "Fold: 10/10... Epoch: 35/1000... Loss: 0.501623... Val Loss: 0.633380... Accuracy: 0.621795\n",
      "Fold: 10/10... Epoch: 36/1000... Loss: 0.503381... Val Loss: 0.632516... Accuracy: 0.616667\n",
      "Fold: 10/10... Epoch: 37/1000... Loss: 0.505005... Val Loss: 0.631674... Accuracy: 0.617949\n",
      "Fold: 10/10... Epoch: 38/1000... Loss: 0.506543... Val Loss: 0.630843... Accuracy: 0.620513\n",
      "Fold: 10/10... Epoch: 39/1000... Loss: 0.507775... Val Loss: 0.630041... Accuracy: 0.623077\n",
      "Fold: 10/10... Epoch: 40/1000... Loss: 0.509031... Val Loss: 0.629297... Accuracy: 0.625641\n",
      "Fold: 10/10... Epoch: 41/1000... Loss: 0.509614... Val Loss: 0.628607... Accuracy: 0.625641\n",
      "Fold: 10/10... Epoch: 42/1000... Loss: 0.510052... Val Loss: 0.627976... Accuracy: 0.624359\n",
      "Fold: 10/10... Epoch: 43/1000... Loss: 0.510271... Val Loss: 0.627374... Accuracy: 0.626923\n",
      "Fold: 10/10... Epoch: 44/1000... Loss: 0.510187... Val Loss: 0.626845... Accuracy: 0.626923\n",
      "Fold: 10/10... Epoch: 45/1000... Loss: 0.509976... Val Loss: 0.626348... Accuracy: 0.628205\n",
      "Fold: 10/10... Epoch: 46/1000... Loss: 0.509248... Val Loss: 0.625906... Accuracy: 0.625641\n",
      "Fold: 10/10... Epoch: 47/1000... Loss: 0.508389... Val Loss: 0.625436... Accuracy: 0.628205\n",
      "Fold: 10/10... Epoch: 48/1000... Loss: 0.507176... Val Loss: 0.624845... Accuracy: 0.630769\n",
      "Fold: 10/10... Epoch: 49/1000... Loss: 0.504602... Val Loss: 0.624405... Accuracy: 0.635897\n",
      "Fold: 10/10... Epoch: 50/1000... Loss: 0.501784... Val Loss: 0.624011... Accuracy: 0.641026\n",
      "Fold: 10/10... Epoch: 51/1000... Loss: 0.499090... Val Loss: 0.623740... Accuracy: 0.644872\n",
      "Fold: 10/10... Epoch: 52/1000... Loss: 0.497496... Val Loss: 0.623301... Accuracy: 0.642308\n",
      "Fold: 10/10... Epoch: 53/1000... Loss: 0.495308... Val Loss: 0.622740... Accuracy: 0.643590\n",
      "Fold: 10/10... Epoch: 54/1000... Loss: 0.492071... Val Loss: 0.622764... Accuracy: 0.638462\n",
      "Fold: 10/10... Epoch: 55/1000... Loss: 0.490393... Val Loss: 0.622668... Accuracy: 0.638462\n",
      "Fold: 10/10... Epoch: 56/1000... Loss: 0.488663... Val Loss: 0.622083... Accuracy: 0.639744\n",
      "Fold: 10/10... Epoch: 57/1000... Loss: 0.484464... Val Loss: 0.621528... Accuracy: 0.641026\n",
      "Fold: 10/10... Epoch: 58/1000... Loss: 0.480299... Val Loss: 0.621715... Accuracy: 0.639744\n",
      "Fold: 10/10... Epoch: 59/1000... Loss: 0.479227... Val Loss: 0.621696... Accuracy: 0.643590\n",
      "Fold: 10/10... Epoch: 60/1000... Loss: 0.477904... Val Loss: 0.621511... Accuracy: 0.642308\n",
      "Fold: 10/10... Epoch: 61/1000... Loss: 0.476455... Val Loss: 0.621266... Accuracy: 0.641026\n",
      "Fold: 10/10... Epoch: 62/1000... Loss: 0.474563... Val Loss: 0.621031... Accuracy: 0.647436\n",
      "Fold: 10/10... Epoch: 63/1000... Loss: 0.472838... Val Loss: 0.620798... Accuracy: 0.646154\n",
      "Fold: 10/10... Epoch: 64/1000... Loss: 0.470816... Val Loss: 0.620571... Accuracy: 0.647436\n",
      "Fold: 10/10... Epoch: 65/1000... Loss: 0.468841... Val Loss: 0.620359... Accuracy: 0.648718\n",
      "Fold: 10/10... Epoch: 66/1000... Loss: 0.466685... Val Loss: 0.620146... Accuracy: 0.652564\n",
      "Fold: 10/10... Epoch: 67/1000... Loss: 0.464361... Val Loss: 0.619947... Accuracy: 0.655128\n",
      "Fold: 10/10... Epoch: 68/1000... Loss: 0.461825... Val Loss: 0.619746... Accuracy: 0.655128\n",
      "Fold: 10/10... Epoch: 69/1000... Loss: 0.459142... Val Loss: 0.619582... Accuracy: 0.656410\n",
      "Fold: 10/10... Epoch: 70/1000... Loss: 0.456386... Val Loss: 0.619330... Accuracy: 0.657692\n",
      "Fold: 10/10... Epoch: 71/1000... Loss: 0.453094... Val Loss: 0.618947... Accuracy: 0.657692\n",
      "Fold: 10/10... Epoch: 72/1000... Loss: 0.448780... Val Loss: 0.618514... Accuracy: 0.656410\n",
      "Fold: 10/10... Epoch: 73/1000... Loss: 0.444366... Val Loss: 0.618674... Accuracy: 0.656410\n",
      "Fold: 10/10... Epoch: 74/1000... Loss: 0.442439... Val Loss: 0.619131... Accuracy: 0.656410\n",
      "Fold: 10/10... Epoch: 75/1000... Loss: 0.441372... Val Loss: 0.619159... Accuracy: 0.651282\n",
      "Fold: 10/10... Epoch: 76/1000... Loss: 0.440024... Val Loss: 0.618990... Accuracy: 0.650000\n",
      "Fold: 10/10... Epoch: 77/1000... Loss: 0.438323... Val Loss: 0.618807... Accuracy: 0.648718\n",
      "Fold: 10/10... Epoch: 78/1000... Loss: 0.436233... Val Loss: 0.618537... Accuracy: 0.655128\n",
      "Fold: 10/10... Epoch: 79/1000... Loss: 0.432791... Val Loss: 0.617787... Accuracy: 0.657692\n",
      "Fold: 10/10... Epoch: 80/1000... Loss: 0.428390... Val Loss: 0.618120... Accuracy: 0.652564\n",
      "Fold: 10/10... Epoch: 81/1000... Loss: 0.426668... Val Loss: 0.618685... Accuracy: 0.655128\n",
      "Fold: 10/10... Epoch: 82/1000... Loss: 0.425380... Val Loss: 0.618684... Accuracy: 0.652564\n",
      "Fold: 10/10... Epoch: 83/1000... Loss: 0.424212... Val Loss: 0.618584... Accuracy: 0.648718\n",
      "Fold: 10/10... Epoch: 84/1000... Loss: 0.422762... Val Loss: 0.618497... Accuracy: 0.647436\n",
      "Fold: 10/10... Epoch: 85/1000... Loss: 0.421013... Val Loss: 0.618410... Accuracy: 0.650000\n",
      "Fold: 10/10... Epoch: 86/1000... Loss: 0.419093... Val Loss: 0.618333... Accuracy: 0.647436\n",
      "Fold: 10/10... Epoch: 87/1000... Loss: 0.417103... Val Loss: 0.618275... Accuracy: 0.647436\n",
      "Fold: 10/10... Epoch: 88/1000... Loss: 0.414952... Val Loss: 0.618220... Accuracy: 0.648718\n",
      "Fold: 10/10... Epoch: 89/1000... Loss: 0.412663... Val Loss: 0.618185... Accuracy: 0.648718\n",
      "Early stopping now... Min val loss: 0.617787... Max acc: 0.657692\n"
     ]
    }
   ],
   "source": [
    "all_accs, all_losses = train_model(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy across folds: 0.639571... Mean val loss across folds: 0.638833\n",
      "Best accuracy across folds: 0.668281... Lowest val loss across folds: 0.617787\n",
      "Worst accuracy across folds: 0.614137... Highest val loss across folds: 0.659856\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean accuracy across folds: {:.6f}...\".format(np.mean(all_accs)),\n",
    "      \"Mean val loss across folds: {:.6f}\".format(np.mean(all_losses)))\n",
    "\n",
    "print(\"Best accuracy across folds: {:.6f}...\".format(np.max(all_accs)),\n",
    "      \"Lowest val loss across folds: {:.6f}\".format(np.min(all_losses)))\n",
    "\n",
    "print(\"Worst accuracy across folds: {:.6f}...\".format(np.min(all_accs)),\n",
    "      \"Highest val loss across folds: {:.6f}\".format(np.max(all_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy should be around 64%, which is comparable to what I obtained with the SVM model (66%). Not too bad given the small number of trials. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "741.917px",
    "left": "1647px",
    "right": "20px",
    "top": "153px",
    "width": "474.067px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
